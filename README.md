# CSC-249-Final-Project
Hallucinations and Fairness in AI Models: A Survey of Recent Developments

Abstract

Recent advances in vision and vision‑language models have unlocked impressive zero‑shot and generative capabilities, yet they also expose two critical reliability challenges: hallucination — the production of content not grounded in the input, and fairness — systematic performance disparities across demographic or contextual subgroups. Although each phenomenon has been studied in isolation, their underlying causes often intertwine: spurious correlations in training data can simultaneously induce ungrounded outputs and biased predictions. This survey provides the first integrative review of hallucination and fairness in modern AI systems. We formalize both concepts, summarize evaluation metrics, and propose a unified taxonomy spanning three dimensions: \textit{task} (hallucination detection/mitigation, bias discovery, subgroup fairness, adversarial robustness), \textit{model} (foundation vision‑language models, standard image classifiers, adversarially trained frameworks), and \textit{dataset} (COCO, Visual Genome, CIFAR‑100, BREEDS, ImageNet variants). Drawing on more than fifty recent papers—including empirical studies such as Can CLIP Count Stars?, DIM, DBD, Bag‑of‑Tricks, and L2T—we analyze prevailing mitigation strategies, highlight trade‑offs between factuality and equity, and identify open challenges such as incomplete ground truth and metric misalignment. We conclude by outlining future directions toward unified benchmarks and reliability‑aware training protocols, aiming to foster AI systems that are both truthful and equitable in real‑world deployment.
